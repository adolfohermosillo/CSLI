{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4b69a1",
   "metadata": {},
   "source": [
    "# End-to-End Neural Machine Machine Translation System "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743bed39",
   "metadata": {},
   "source": [
    "In this notebook we will learn how to train a neural machine transaltion system from scracth. We will use a bidirectional LSTM with attention encoder-decoder network to translate between two languages (from English, Spanish, or Wixárika, an indigenous langauge of Mexico). The code in this notebook is adapted from https://opennmt.net/OpenNMT-py/examples/Library.html.\n",
    "\n",
    "### Deliverable\n",
    "\n",
    "A 1 page report describing model behavior for at least 2 the following 3 experiments: \n",
    "\n",
    "\n",
    "1. Spanish --> English , English --> Spanish\n",
    "1. English --> Wixárika, Wixárika --> English\n",
    "1. Spanish --> Wixárika, Wixárika --> Spanish\n",
    "\n",
    "\n",
    "Which language pair is easier to translate?  How may the data influence this behavior? Can you think of creative ways to improve model performance? If you do any type of hyperparameter exploration make sure to mention what you did (e.g., changing the number of layers in the encoder/decoder, the size of the word embeddings, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24835a21",
   "metadata": {},
   "source": [
    "## Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94251e8",
   "metadata": {},
   "source": [
    "Create a conda environment following [this notebook](https://github.com/cgpotts/cs224u/blob/master/setup.ipynb), activate the environment, and download the following two packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Machine Translation framework\n",
    "#!pip install openNMT-py \n",
    "\n",
    "# Evaluation metrics\n",
    "#!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66d30e",
   "metadata": {},
   "source": [
    "We now begin our NMT journey by importing a few modules and functions that will be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "from onmt.inputters.inputter import _load_vocab, _build_fields_vocab, get_fields, IterOnDevice\n",
    "from onmt.inputters.corpus import ParallelCorpus\n",
    "from onmt.inputters.dynamic_iterator import DynamicDatasetIter\n",
    "from onmt.translate import GNMTGlobalScorer, Translator, TranslationBuilder\n",
    "from onmt.utils.misc import set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414991b8",
   "metadata": {},
   "source": [
    "Enable logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14199f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable logging\n",
    "from onmt.utils.logging import init_logger, logger\n",
    "init_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc7ed47",
   "metadata": {},
   "source": [
    "Set random seed and check whether we have access to a gpu (it's okay if we don't have one, a cpu will be sufficient for this notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59364dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "set_random_seed(1111, is_cuda)\n",
    "is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a4972",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "An NMT system uses a parallel corpus, sometimes called a bitext. In this notebook, we will use a small parallel corpus of Spanish and [Wixárika](https://en.wikipedia.org/wiki/Huichol_language). The Wixárika language is spoken in Western Mexico. This language is also commonly known as Huichol, however, the Wixáritari people prefer the term Wixárika to refer to their language. The parallel corpus is a collection of Wixárika sentences, and their Spanish translations, from a descriptive grammar [(Gomez, 1999)](https://arqueologiamexicana.mx/sites/default/files/banco_imagenes/huichol-de-san-andres-cohamiata.pdf). We have included an English translation of the corpus for you to play around and get a hold of the task and and data.\n",
    "\n",
    "We specify the source and target languages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34491230",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'wixarika'\n",
    "target =  'spanish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = open(f'toy-data/{source}.txt','r').readlines()\n",
    "target_data = open(f'toy-data/{target}.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac357e80",
   "metadata": {},
   "source": [
    "Since there aren't  predetermined data splits for this corpus, we will create our own. We will use 90% for training and 10% for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [i for i in source_data]\n",
    "y = [i for i in target_data]\n",
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'src': {'train':X_train,'val':X_val,},\n",
    "'tgt': {'train':y_train,'val':y_val}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d0590",
   "metadata": {},
   "source": [
    "We write the new data splits in the `toy-data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in datasets:\n",
    "    for split in datasets[lang]:\n",
    "        with open (f'toy-data/{lang}-{split}.txt','w') as f:\n",
    "            f.write(\"\".join(datasets[lang][split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls toy-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c3221",
   "metadata": {},
   "source": [
    "As for any use case of OpenNMT-py 2.0, we can start by creating a simple YAML configuration with our datasets. This is the easiest way to build the proper opts Namespace that will be used to create the vocabulary(ies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c52a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_config = \"\"\"\n",
    "## Where the samples will be written\n",
    "save_data: toy-data/run/example\n",
    "## Where the vocab(s) will be written\n",
    "src_vocab: toy-data/run/example.vocab.src\n",
    "tgt_vocab: toy-data/run/example.vocab.tgt\n",
    "# Corpus opts:\n",
    "data:\n",
    "    corpus:\n",
    "        path_src: toy-data/src-train.txt\n",
    "        path_tgt: toy-data/tgt-train.txt\n",
    "        transforms: []\n",
    "        weight: 1\n",
    "    valid:\n",
    "        path_src: toy-data/src-val.txt\n",
    "        path_tgt: toy-data/tgt-val.txt\n",
    "        transforms: []\n",
    "\"\"\"\n",
    "config = yaml.safe_load(yaml_config)\n",
    "with open(\"toy-data/config.yaml\", \"w\") as f:\n",
    "    f.write(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a73ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.utils.parse import ArgumentParser\n",
    "parser = ArgumentParser(description='build_vocab.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db42a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.opts import dynamic_prepare_opts\n",
    "dynamic_prepare_opts(parser, build_vocab_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b677698",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_args = ([\"-config\", \"toy-data/config.yaml\", \"-n_sample\", \"10000\"])\n",
    "opts, unknown = parser.parse_known_args(base_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d210643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will delete vocabularies if there are any, \n",
    "# otherwise you may run into an error in the cell below\n",
    "!rm toy-data/run/example.vocab.src\n",
    "!rm toy-data/run/example.vocab.tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onmt.bin.build_vocab import build_vocab_main\n",
    "build_vocab_main(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00afce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls toy-data/run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7559f92",
   "metadata": {},
   "source": [
    "We just created our source and target vocabularies, respectively toy-data/run/example.vocab.src and toy-data/run/example.vocab.tgt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155217e3",
   "metadata": {},
   "source": [
    "### Build fields\n",
    "We can build the fields from the text files that were just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_path = \"toy-data/run/example.vocab.src\"\n",
    "tgt_vocab_path = \"toy-data/run/example.vocab.tgt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502780bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the frequency counter\n",
    "counters = defaultdict(Counter)\n",
    "# load source vocab\n",
    "_src_vocab, _src_vocab_size = _load_vocab(\n",
    "    src_vocab_path,\n",
    "    'src',\n",
    "    counters)\n",
    "# load target vocab\n",
    "_tgt_vocab, _tgt_vocab_size = _load_vocab(\n",
    "    tgt_vocab_path,\n",
    "    'tgt',\n",
    "    counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0161361",
   "metadata": {},
   "source": [
    "Let's take a look at the most frequent tokens in the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c92ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source vocab\n",
    "_src_vocab[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_src_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e165dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target vocab\n",
    "_tgt_vocab[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tgt_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fields\n",
    "src_nfeats, tgt_nfeats = 0, 0 # do not support word features for now\n",
    "fields = get_fields(\n",
    "    'text', src_nfeats, tgt_nfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba4e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0951670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build fields vocab\n",
    "share_vocab = False\n",
    "vocab_size_multiple = 1\n",
    "src_vocab_size = 2000\n",
    "tgt_vocab_size = 2000\n",
    "src_words_min_frequency = 1\n",
    "tgt_words_min_frequency = 1\n",
    "vocab_fields = _build_fields_vocab(\n",
    "    fields, counters, 'text', share_vocab,\n",
    "    vocab_size_multiple,\n",
    "    src_vocab_size, src_words_min_frequency,\n",
    "    tgt_vocab_size, tgt_words_min_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367e9726",
   "metadata": {},
   "source": [
    "An alternative way of creating these fields is to run `onmt_train` without actually training, to just output the necessary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff0f5b",
   "metadata": {},
   "source": [
    "### Prepare for training: model and optimizer creation\n",
    "Let’s get a few fields/vocab related variables to simplify the model creation a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8a057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text_field = vocab_fields[\"src\"].base_field\n",
    "src_vocab = src_text_field.vocab\n",
    "src_padding = src_vocab.stoi[src_text_field.pad_token]\n",
    "\n",
    "tgt_text_field = vocab_fields['tgt'].base_field\n",
    "tgt_vocab = tgt_text_field.vocab\n",
    "tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b00fb",
   "metadata": {},
   "source": [
    "### Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28580e",
   "metadata": {},
   "source": [
    "Next we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs (more specifically LSTMs) and the encoder will be bidirectional. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfe532",
   "metadata": {},
   "source": [
    "### Core Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ad232",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 500\n",
    "rnn_size = 500 \n",
    "## encoder \n",
    "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),\n",
    "                                             word_padding_idx=src_padding)\n",
    "\n",
    "encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,\n",
    "                                   rnn_type=\"LSTM\", bidirectional=True,\n",
    "                                   embeddings=encoder_embeddings)\n",
    "\n",
    "## decoder \n",
    "decoder_embeddings = onmt.modules.Embeddings(emb_size,\n",
    "                                             len(tgt_vocab),\n",
    "                                             word_padding_idx=tgt_padding)\n",
    "\n",
    "decoder = onmt.decoders.decoder.InputFeedRNNDecoder( hidden_size=rnn_size, \n",
    "                                                    num_layers=1, \n",
    "                                                    bidirectional_encoder=True, \n",
    "                                                    rnn_type=\"LSTM\", \n",
    "                                                    embeddings=decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2502d60",
   "metadata": {},
   "source": [
    "Putting the model in the gpu or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea88dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = onmt.models.model.NMTModel(encoder, decoder)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8002ed",
   "metadata": {},
   "source": [
    "### Word generator & loss module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the tgt word generator and loss computation module\n",
    "\n",
    "model.generator = nn.Sequential(\n",
    "    nn.Linear(rnn_size, len(tgt_vocab)),\n",
    "    nn.LogSoftmax(dim=-1)).to(device)\n",
    "\n",
    "loss = onmt.utils.loss.NMTLossCompute(\n",
    "    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=\"sum\"),\n",
    "    generator=model.generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8729b3",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed72044",
   "metadata": {},
   "source": [
    "Now we set up the optimizer. This could be a core torch optim class, or our wrapper which handles learning rate updates and gradient normalization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35403aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1\n",
    "torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optim = onmt.utils.optimizers.Optimizer(\n",
    "    torch_optimizer, learning_rate=lr, max_grad_norm=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03406fc3",
   "metadata": {},
   "source": [
    "### Create the training and validation data iterators\n",
    "\n",
    "Now we need to create the dynamic dataset iterator. This is not very ‘library-friendly’ for now because of the way the DynamicDatasetIter constructor is defined. It may evolve in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = \"toy-data/src-train.txt\"\n",
    "tgt_train = \"toy-data/tgt-train.txt\"\n",
    "src_val = \"toy-data/src-val.txt\"\n",
    "tgt_val = \"toy-data/tgt-val.txt\"\n",
    "\n",
    "# build the ParallelCorpus\n",
    "corpus = ParallelCorpus(\"corpus\", src_train, tgt_train)\n",
    "valid = ParallelCorpus(\"valid\", src_val, tgt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the training iterator\n",
    "train_iter = DynamicDatasetIter(\n",
    "    corpora={\"corpus\": corpus},\n",
    "    corpora_info={\"corpus\": {\"weight\": 1}},\n",
    "    transforms={},\n",
    "    fields=vocab_fields,\n",
    "    is_train=True,\n",
    "    batch_type=\"tokens\",\n",
    "    batch_size=128,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f833a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the iteration happens on GPU 0 (-1 for CPU, N for GPU N)\n",
    "train_iter = iter(IterOnDevice(train_iter, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the validation iterator\n",
    "valid_iter = DynamicDatasetIter(\n",
    "    corpora={\"valid\": valid},\n",
    "    corpora_info={\"valid\": {\"weight\": 1}},\n",
    "    transforms={},\n",
    "    fields=vocab_fields,\n",
    "    is_train=False,\n",
    "    batch_type=\"sents\",\n",
    "    batch_size=8,\n",
    "    batch_size_multiple=1,\n",
    "    data_type=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have a gpu, we set -1 to 0\n",
    "valid_iter = IterOnDevice(valid_iter, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be94d835",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We specify some training options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5513f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## play around with val_every\n",
    "val_every = 50\n",
    "report_manager = onmt.utils.ReportMgr(\n",
    "    report_every=val_every, start_time=None, tensorboard_writer=None)\n",
    "\n",
    "trainer = onmt.Trainer(model=model,\n",
    "                       train_loss=loss,\n",
    "                       valid_loss=loss,\n",
    "                       optim=optim,\n",
    "                       report_manager=report_manager,\n",
    "                       dropout=[0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa3969",
   "metadata": {},
   "source": [
    "Finally we train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## play around with train_steps\n",
    "trainer.train(train_iter=train_iter,\n",
    "              train_steps=1000,\n",
    "              valid_iter=valid_iter,\n",
    "              valid_steps=val_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d6c70",
   "metadata": {},
   "source": [
    "# Translate \n",
    "For translation, we can build a “traditional” (as opposed to dynamic) dataset for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f471b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": src_val,   \"features\": {} }\n",
    "tgt_data = {\"reader\": onmt.inputters.str2reader[\"text\"](), \"data\": tgt_val,   \"features\": {} }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82868cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_readers, _data = onmt.inputters.Dataset.config(\n",
    "    [('src', src_data), ('tgt', tgt_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667685a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = onmt.inputters.Dataset(\n",
    "    vocab_fields, readers=_readers, data=_data,\n",
    "    sort_key=onmt.inputters.str2sortkey[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caee4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = onmt.inputters.OrderedIterator(\n",
    "            dataset=dataset,\n",
    "            device=device,\n",
    "            batch_size=10,\n",
    "            train=False,\n",
    "            sort=False,\n",
    "            sort_within_batch=True,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_reader = onmt.inputters.str2reader[\"text\"]\n",
    "tgt_reader = onmt.inputters.str2reader[\"text\"]\n",
    "scorer = GNMTGlobalScorer(alpha=0.7, \n",
    "                          beta=0., \n",
    "                          length_penalty=\"avg\", \n",
    "                          coverage_penalty=\"none\")\n",
    "gpu = 0 if torch.cuda.is_available() else -1\n",
    "translator = Translator(model=model, \n",
    "                        fields=vocab_fields, \n",
    "                        src_reader=src_reader, \n",
    "                        tgt_reader=tgt_reader, \n",
    "                        global_scorer=scorer,\n",
    "                        gpu=gpu)\n",
    "builder = onmt.translate.TranslationBuilder(data=dataset, \n",
    "                                            fields=vocab_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26183285",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve = lambda x:[ i.split(':') for i in x.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d41f12",
   "metadata": {},
   "source": [
    "Note: translations will be very poor, because of the very low quantity of data, the absence of proper tokenization, and the brevity of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ebfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = []\n",
    "for batch in data_iter:\n",
    "    trans_batch = translator.translate_batch(\n",
    "        batch=batch, src_vocabs=[src_vocab],\n",
    "        attn_debug=False)\n",
    "    translations = builder.from_batch(trans_batch)\n",
    "    for trans in translations:\n",
    "        \n",
    "        a=trans.log(0)\n",
    "        print(a)\n",
    "        source_sentence = ((retrieve(a))[1][1])\n",
    "        prediction = ((retrieve(a))[2][1])\n",
    "        predicted_output.append(prediction)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92dfc8a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a0ae2",
   "metadata": {},
   "source": [
    "\n",
    "We will use two evaluation metrics for machine translation: [BLEU](https://aclanthology.org/P02-1040.pdf) and [CHRF](https://aclanthology.org/W17-4770.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba39c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import BLEU, CHRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7470b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = BLEU()\n",
    "chrf = CHRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = open(tgt_val,'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645575c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = [i for i in zip(gold,predicted_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(da, columns=['gold','prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_ = bleu.corpus_score(predicted_output,[gold])\n",
    "print(bleu_.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f267e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf_ = chrf.corpus_score(predicted_output,gold)\n",
    "print(chrf_.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
